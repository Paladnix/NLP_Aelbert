07-25 10:19 root     INFO     Namespace(add_crowd=False, batch_size=1000, data_setup='single', dev_data='ontonotes/g_dev.json', dim_hidden=100, enhanced_mention=False, eval_batch_size=1998, eval_data='ontonotes/g_test.json', eval_period=500, goal='open', gpu=False, input_dropout=0.2, learning_rate=0.001, load=True, log_period=1000, lstm_type='two', mention_dropout=0.5, mode='train', model_id='paws-train', multitask=False, num_epoch=10, only_crowd=False, reload_model_name='./_model/qqp-paws-train', remove_el=False, remove_open=False, rnn_dim=100, save_period=1000, seed=1777, train_data='ontonotes/augmented_train.json')
07-25 10:19 transformers.tokenization_utils_base INFO     loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/ybai/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
07-25 10:19 root     INFO     数据集大小: 11988
07-25 10:19 transformers.configuration_utils INFO     loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/ybai/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517
07-25 10:19 transformers.configuration_utils INFO     Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

07-25 10:19 transformers.modeling_utils INFO     loading weights file https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin from cache at /home/ybai/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
07-25 10:20 transformers.modeling_utils INFO     All model checkpoint weights were used when initializing BertModel.

07-25 10:20 transformers.modeling_utils INFO     All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertModel for predictions without further training.
07-25 10:20 transformers.configuration_utils INFO     loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/ybai/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517
07-25 10:20 transformers.configuration_utils INFO     Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

07-25 10:20 transformers.modeling_utils INFO     loading weights file https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin from cache at /home/ybai/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
07-25 10:20 transformers.modeling_utils INFO     All model checkpoint weights were used when initializing BertModel.

07-25 10:20 transformers.modeling_utils INFO     All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertModel for predictions without further training.
07-25 10:20 root     INFO     load model from: ./_model/qqp-paws-train
07-25 10:20 root     INFO     The 0 epoch.
07-25 10:21 transformers.tokenization_utils ERROR    We need to remove 625 to truncate the inputbut the first sequence has a length 373. Please select another truncation strategy than TruncationStrategy.ONLY_FIRST, for instance 'longest_first' or 'only_second'.
07-25 10:21 transformers.tokenization_utils ERROR    We need to remove 455 to truncate the inputbut the first sequence has a length 290. Please select another truncation strategy than TruncationStrategy.ONLY_FIRST, for instance 'longest_first' or 'only_second'.
07-25 10:21 transformers.tokenization_utils ERROR    We need to remove 183 to truncate the inputbut the first sequence has a length 154. Please select another truncation strategy than TruncationStrategy.ONLY_FIRST, for instance 'longest_first' or 'only_second'.
07-25 10:24 root     INFO     Train
07-25 10:24 root     INFO     tloss: 0.2054
07-25 10:24 root     INFO     acc:   0.9161
07-25 10:24 root     INFO     auc:   0.8980
07-25 10:24 root     INFO     f1:    0.8644
07-25 10:24 transformers.tokenization_utils_base INFO     loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/ybai/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
07-25 10:24 root     INFO     数据集大小: 677
07-25 10:24 root     INFO     DEV_AND_TEST
07-25 10:24 root     INFO     tloss: 0.4795
07-25 10:24 root     INFO     acc:   0.8242
07-25 10:24 root     INFO     auc:   0.8283
07-25 10:24 root     INFO     f1:    0.7289
07-25 10:24 root     INFO     The 1 epoch.
07-25 10:24 transformers.tokenization_utils ERROR    We need to remove 183 to truncate the inputbut the first sequence has a length 154. Please select another truncation strategy than TruncationStrategy.ONLY_FIRST, for instance 'longest_first' or 'only_second'.
07-25 10:25 transformers.tokenization_utils ERROR    We need to remove 625 to truncate the inputbut the first sequence has a length 373. Please select another truncation strategy than TruncationStrategy.ONLY_FIRST, for instance 'longest_first' or 'only_second'.
07-25 10:26 transformers.tokenization_utils ERROR    We need to remove 455 to truncate the inputbut the first sequence has a length 290. Please select another truncation strategy than TruncationStrategy.ONLY_FIRST, for instance 'longest_first' or 'only_second'.
07-25 10:28 root     INFO     Train
07-25 10:28 root     INFO     tloss: 0.1961
07-25 10:28 root     INFO     acc:   0.9200
07-25 10:28 root     INFO     auc:   0.9021
07-25 10:28 root     INFO     f1:    0.8706
07-25 10:28 transformers.tokenization_utils_base INFO     loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/ybai/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
07-25 10:28 root     INFO     数据集大小: 677
07-25 10:28 root     INFO     DEV_AND_TEST
07-25 10:28 root     INFO     tloss: 0.6478
07-25 10:28 root     INFO     acc:   0.7681
07-25 10:28 root     INFO     auc:   0.8051
07-25 10:28 root     INFO     f1:    0.6841
07-25 10:28 root     INFO     The 2 epoch.
07-25 10:28 transformers.tokenization_utils ERROR    We need to remove 625 to truncate the inputbut the first sequence has a length 373. Please select another truncation strategy than TruncationStrategy.ONLY_FIRST, for instance 'longest_first' or 'only_second'.
07-25 10:29 transformers.tokenization_utils ERROR    We need to remove 183 to truncate the inputbut the first sequence has a length 154. Please select another truncation strategy than TruncationStrategy.ONLY_FIRST, for instance 'longest_first' or 'only_second'.
07-25 10:30 transformers.tokenization_utils ERROR    We need to remove 455 to truncate the inputbut the first sequence has a length 290. Please select another truncation strategy than TruncationStrategy.ONLY_FIRST, for instance 'longest_first' or 'only_second'.
07-25 10:32 root     INFO     Train
07-25 10:32 root     INFO     tloss: 0.1877
07-25 10:32 root     INFO     acc:   0.9256
07-25 10:32 root     INFO     auc:   0.9097
07-25 10:32 root     INFO     f1:    0.8801
07-25 10:32 transformers.tokenization_utils_base INFO     loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/ybai/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
07-25 10:32 root     INFO     数据集大小: 677
07-25 10:32 root     INFO     DEV_AND_TEST
07-25 10:32 root     INFO     tloss: 0.5106
07-25 10:32 root     INFO     acc:   0.8154
07-25 10:32 root     INFO     auc:   0.8269
07-25 10:32 root     INFO     f1:    0.7228
07-25 10:33 root     INFO     The 3 epoch.
07-25 10:33 transformers.tokenization_utils ERROR    We need to remove 455 to truncate the inputbut the first sequence has a length 290. Please select another truncation strategy than TruncationStrategy.ONLY_FIRST, for instance 'longest_first' or 'only_second'.
07-25 10:34 transformers.tokenization_utils ERROR    We need to remove 625 to truncate the inputbut the first sequence has a length 373. Please select another truncation strategy than TruncationStrategy.ONLY_FIRST, for instance 'longest_first' or 'only_second'.
07-25 10:36 transformers.tokenization_utils ERROR    We need to remove 183 to truncate the inputbut the first sequence has a length 154. Please select another truncation strategy than TruncationStrategy.ONLY_FIRST, for instance 'longest_first' or 'only_second'.
07-25 10:36 root     INFO     Train
07-25 10:36 root     INFO     tloss: 0.1731
07-25 10:36 root     INFO     acc:   0.9301
07-25 10:36 root     INFO     auc:   0.9147
07-25 10:36 root     INFO     f1:    0.8872
07-25 10:37 transformers.tokenization_utils_base INFO     loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/ybai/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
07-25 10:37 root     INFO     数据集大小: 677
07-25 10:37 root     INFO     DEV_AND_TEST
07-25 10:37 root     INFO     tloss: 0.5896
07-25 10:37 root     INFO     acc:   0.8080
07-25 10:37 root     INFO     auc:   0.8281
07-25 10:37 root     INFO     f1:    0.7198
07-25 10:37 root     INFO     The 4 epoch.
07-25 10:38 transformers.tokenization_utils ERROR    We need to remove 625 to truncate the inputbut the first sequence has a length 373. Please select another truncation strategy than TruncationStrategy.ONLY_FIRST, for instance 'longest_first' or 'only_second'.
07-25 10:39 transformers.tokenization_utils ERROR    We need to remove 183 to truncate the inputbut the first sequence has a length 154. Please select another truncation strategy than TruncationStrategy.ONLY_FIRST, for instance 'longest_first' or 'only_second'.
07-25 10:40 transformers.tokenization_utils ERROR    We need to remove 455 to truncate the inputbut the first sequence has a length 290. Please select another truncation strategy than TruncationStrategy.ONLY_FIRST, for instance 'longest_first' or 'only_second'.
07-25 10:41 root     INFO     Train
07-25 10:41 root     INFO     tloss: 0.1673
07-25 10:41 root     INFO     acc:   0.9332
07-25 10:41 root     INFO     auc:   0.9198
07-25 10:41 root     INFO     f1:    0.8928
07-25 10:41 transformers.tokenization_utils_base INFO     loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/ybai/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
07-25 10:41 root     INFO     数据集大小: 677
07-25 10:41 root     INFO     DEV_AND_TEST
07-25 10:41 root     INFO     tloss: 0.5905
07-25 10:41 root     INFO     acc:   0.8006
07-25 10:41 root     INFO     auc:   0.8230
07-25 10:41 root     INFO     f1:    0.7122
07-25 10:41 root     INFO     The 5 epoch.
07-25 10:42 transformers.tokenization_utils ERROR    We need to remove 625 to truncate the inputbut the first sequence has a length 373. Please select another truncation strategy than TruncationStrategy.ONLY_FIRST, for instance 'longest_first' or 'only_second'.
07-25 10:42 transformers.tokenization_utils ERROR    We need to remove 183 to truncate the inputbut the first sequence has a length 154. Please select another truncation strategy than TruncationStrategy.ONLY_FIRST, for instance 'longest_first' or 'only_second'.
07-25 10:43 transformers.tokenization_utils ERROR    We need to remove 455 to truncate the inputbut the first sequence has a length 290. Please select another truncation strategy than TruncationStrategy.ONLY_FIRST, for instance 'longest_first' or 'only_second'.
07-25 10:45 root     INFO     Train
07-25 10:45 root     INFO     tloss: 0.1559
07-25 10:45 root     INFO     acc:   0.9403
07-25 10:45 root     INFO     auc:   0.9277
07-25 10:45 root     INFO     f1:    0.9041
07-25 10:45 transformers.tokenization_utils_base INFO     loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/ybai/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
07-25 10:45 root     INFO     数据集大小: 677
07-25 10:45 root     INFO     DEV_AND_TEST
07-25 10:45 root     INFO     tloss: 0.6786
07-25 10:45 root     INFO     acc:   0.7799
07-25 10:45 root     INFO     auc:   0.8165
07-25 10:45 root     INFO     f1:    0.6978
07-25 10:45 root     INFO     The 6 epoch.
07-25 10:45 transformers.tokenization_utils ERROR    We need to remove 625 to truncate the inputbut the first sequence has a length 373. Please select another truncation strategy than TruncationStrategy.ONLY_FIRST, for instance 'longest_first' or 'only_second'.
07-25 10:45 transformers.tokenization_utils ERROR    We need to remove 183 to truncate the inputbut the first sequence has a length 154. Please select another truncation strategy than TruncationStrategy.ONLY_FIRST, for instance 'longest_first' or 'only_second'.
07-25 10:48 transformers.tokenization_utils ERROR    We need to remove 455 to truncate the inputbut the first sequence has a length 290. Please select another truncation strategy than TruncationStrategy.ONLY_FIRST, for instance 'longest_first' or 'only_second'.
07-25 10:49 root     INFO     Train
07-25 10:49 root     INFO     tloss: 0.1506
07-25 10:49 root     INFO     acc:   0.9416
07-25 10:49 root     INFO     auc:   0.9302
07-25 10:49 root     INFO     f1:    0.9066
07-25 10:49 transformers.tokenization_utils_base INFO     loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/ybai/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
07-25 10:49 root     INFO     数据集大小: 677
07-25 10:49 root     INFO     DEV_AND_TEST
07-25 10:49 root     INFO     tloss: 0.5537
07-25 10:49 root     INFO     acc:   0.8213
07-25 10:49 root     INFO     auc:   0.8326
07-25 10:49 root     INFO     f1:    0.7305
07-25 10:49 root     INFO     The 7 epoch.
07-25 10:49 transformers.tokenization_utils ERROR    We need to remove 625 to truncate the inputbut the first sequence has a length 373. Please select another truncation strategy than TruncationStrategy.ONLY_FIRST, for instance 'longest_first' or 'only_second'.
07-25 10:50 transformers.tokenization_utils ERROR    We need to remove 455 to truncate the inputbut the first sequence has a length 290. Please select another truncation strategy than TruncationStrategy.ONLY_FIRST, for instance 'longest_first' or 'only_second'.
07-25 10:52 transformers.tokenization_utils ERROR    We need to remove 183 to truncate the inputbut the first sequence has a length 154. Please select another truncation strategy than TruncationStrategy.ONLY_FIRST, for instance 'longest_first' or 'only_second'.
07-25 10:53 root     INFO     Train
07-25 10:53 root     INFO     tloss: 0.1373
07-25 10:53 root     INFO     acc:   0.9465
07-25 10:53 root     INFO     auc:   0.9356
07-25 10:53 root     INFO     f1:    0.9143
07-25 10:53 transformers.tokenization_utils_base INFO     loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/ybai/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
07-25 10:53 root     INFO     数据集大小: 677
07-25 10:53 root     INFO     DEV_AND_TEST
07-25 10:53 root     INFO     tloss: 0.7001
07-25 10:53 root     INFO     acc:   0.7725
07-25 10:53 root     INFO     auc:   0.8082
07-25 10:53 root     INFO     f1:    0.6883
07-25 10:53 root     INFO     The 8 epoch.
07-25 10:53 transformers.tokenization_utils ERROR    We need to remove 625 to truncate the inputbut the first sequence has a length 373. Please select another truncation strategy than TruncationStrategy.ONLY_FIRST, for instance 'longest_first' or 'only_second'.
07-25 10:53 transformers.tokenization_utils ERROR    We need to remove 455 to truncate the inputbut the first sequence has a length 290. Please select another truncation strategy than TruncationStrategy.ONLY_FIRST, for instance 'longest_first' or 'only_second'.
07-25 10:57 transformers.tokenization_utils ERROR    We need to remove 183 to truncate the inputbut the first sequence has a length 154. Please select another truncation strategy than TruncationStrategy.ONLY_FIRST, for instance 'longest_first' or 'only_second'.
07-25 10:57 root     INFO     Train
07-25 10:57 root     INFO     tloss: 0.1265
07-25 10:57 root     INFO     acc:   0.9512
07-25 10:57 root     INFO     auc:   0.9401
07-25 10:57 root     INFO     f1:    0.9215
07-25 10:57 transformers.tokenization_utils_base INFO     loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/ybai/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
07-25 10:57 root     INFO     数据集大小: 677
07-25 10:57 root     INFO     DEV_AND_TEST
07-25 10:57 root     INFO     tloss: 0.7574
07-25 10:57 root     INFO     acc:   0.7770
07-25 10:57 root     INFO     auc:   0.8113
07-25 10:57 root     INFO     f1:    0.6925
07-25 10:58 root     INFO     The 9 epoch.
07-25 10:58 transformers.tokenization_utils ERROR    We need to remove 455 to truncate the inputbut the first sequence has a length 290. Please select another truncation strategy than TruncationStrategy.ONLY_FIRST, for instance 'longest_first' or 'only_second'.
07-25 10:59 transformers.tokenization_utils ERROR    We need to remove 625 to truncate the inputbut the first sequence has a length 373. Please select another truncation strategy than TruncationStrategy.ONLY_FIRST, for instance 'longest_first' or 'only_second'.
07-25 11:01 transformers.tokenization_utils ERROR    We need to remove 183 to truncate the inputbut the first sequence has a length 154. Please select another truncation strategy than TruncationStrategy.ONLY_FIRST, for instance 'longest_first' or 'only_second'.
07-25 11:02 root     INFO     Train
07-25 11:02 root     INFO     tloss: 0.1225
07-25 11:02 root     INFO     acc:   0.9525
07-25 11:02 root     INFO     auc:   0.9442
07-25 11:02 root     INFO     f1:    0.9243
07-25 11:02 transformers.tokenization_utils_base INFO     loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/ybai/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
07-25 11:02 root     INFO     数据集大小: 677
07-25 11:02 root     INFO     DEV_AND_TEST
07-25 11:02 root     INFO     tloss: 0.5822
07-25 11:02 root     INFO     acc:   0.8272
07-25 11:02 root     INFO     auc:   0.8383
07-25 11:02 root     INFO     f1:    0.7383
